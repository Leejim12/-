{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래디언트 소실 & 가중치 초기화\n",
    "parameter 수 적을수록.\n",
    "-. 모델 구조는 알아서\n",
    "-. 같은 구조상에서 더 나은 모델 ==> weight 0인게 많을수록 좋음\n",
    "  ==> 그 parameter는 없는걸로. (선호)\n",
    "-. 초기화 시에도 0에 가까운 값으로 초기화."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import tensorboard\n",
    "\n",
    "#1\n",
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
    "\n",
    "#2 정규화\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "#3 one-hot\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# init = tf.keras.initializers.he_uniform()\n",
    "init = tf.keras.initializers.RandomUniform(0.0,1.0)\n",
    "# act = tf.keras.layers.sigmoid(alpha=0.3)\n",
    "act = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,010\n",
      "Trainable params: 130,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28,28)))\n",
    "model.add(tf.keras.layers.Dense(units=n,activation=act,kernel_initializer=init))\n",
    "model.add(tf.keras.layers.Dense(units=n,activation=act,kernel_initializer=init))\n",
    "model.add(tf.keras.layers.Dense(units=n,activation=act,kernel_initializer=init))\n",
    "model.add(tf.keras.layers.Dense(units=n,activation=act,kernel_initializer=init))\n",
    "model.add(tf.keras.layers.Dense(units=n,activation=act,kernel_initializer=init))\n",
    "model.add(tf.keras.layers.Dense(units=n,activation=act,kernel_initializer=init))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units=10,activation='sigmoid',kernel_initializer=init))\n",
    "\n",
    "model.summary()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"/Users/user/Desktop/\"\n",
    "if not os.path.isdir(path):\n",
    "  os.mkdir(path)\n",
    "log_dir = path+\"3203\"\n",
    "\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/gradient\")\n",
    "file_writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientCallback(tf.keras.callbacks.Callback):\n",
    "  def __init__(self,freq=10):\n",
    "    self.freq = freq\n",
    "    \n",
    "  def on_epoch_end(self,epoch,logs):\n",
    "    if epoch%self.freq != 0:\n",
    "      return\n",
    "    with tf.GradientTape() as tape:\n",
    "      y_pred = model(x_train)\n",
    "      loss = tf.keras.losses.binary_crossentropy(y_train,y_pred)\n",
    "    grads = tape.gradient(loss,model.trainable_weights)\n",
    "    for n in range(len(model.layers)):\n",
    "      i2 = (n-1)*2\n",
    "      i1 = i2+1\n",
    "      \n",
    "      bias_avg = tf.reduce_mean(tf.abs(grads[i1]))\n",
    "      weight_avg = tf.reduce_mean(tf.abs(grads[i2]))\n",
    "      \n",
    "      tf.summary.scalar(\"layer_%d/avg/bias\"%n,data=bias_avg,step=epoch)\n",
    "      tf.summary.scalar(\"layer_%d/avg/weight\"%n,data=weight_avg,step=epoch)\n",
    "      \n",
    "      tf.summary.histogram(\"layer_%d/hist/bias\"%n,data=grads[i1],step=epoch)\n",
    "      tf.summary.histogram(\"layer_%d/hist/weight\"%n,data=grads[i2],step=epoch)\n",
    "    def on_train_end(self,logs):\n",
    "      tf.summary.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/101\n",
      "240/240 - 2s - loss: 2.4585 - accuracy: 0.1001 - val_loss: 2.4013 - val_accuracy: 0.1035 - 2s/epoch - 8ms/step\n",
      "Epoch 2/101\n",
      "240/240 - 1s - loss: 2.4436 - accuracy: 0.1032 - val_loss: 2.3542 - val_accuracy: 0.0997 - 684ms/epoch - 3ms/step\n",
      "Epoch 3/101\n",
      "240/240 - 1s - loss: 2.4382 - accuracy: 0.1044 - val_loss: 2.3794 - val_accuracy: 0.1060 - 688ms/epoch - 3ms/step\n",
      "Epoch 4/101\n",
      "240/240 - 1s - loss: 2.4426 - accuracy: 0.1010 - val_loss: 2.4047 - val_accuracy: 0.0998 - 699ms/epoch - 3ms/step\n",
      "Epoch 5/101\n",
      "240/240 - 1s - loss: 2.4425 - accuracy: 0.0993 - val_loss: 2.4191 - val_accuracy: 0.0975 - 676ms/epoch - 3ms/step\n",
      "Epoch 6/101\n",
      "240/240 - 1s - loss: 2.4423 - accuracy: 0.1031 - val_loss: 2.4092 - val_accuracy: 0.0975 - 661ms/epoch - 3ms/step\n",
      "Epoch 7/101\n",
      "240/240 - 1s - loss: 2.4408 - accuracy: 0.0996 - val_loss: 2.3370 - val_accuracy: 0.1035 - 657ms/epoch - 3ms/step\n",
      "Epoch 8/101\n",
      "240/240 - 1s - loss: 2.4420 - accuracy: 0.1004 - val_loss: 2.5712 - val_accuracy: 0.0997 - 700ms/epoch - 3ms/step\n",
      "Epoch 9/101\n",
      "240/240 - 1s - loss: 2.4438 - accuracy: 0.1016 - val_loss: 2.4939 - val_accuracy: 0.0956 - 663ms/epoch - 3ms/step\n",
      "Epoch 10/101\n",
      "240/240 - 1s - loss: 2.4406 - accuracy: 0.1027 - val_loss: 2.4979 - val_accuracy: 0.1060 - 692ms/epoch - 3ms/step\n",
      "Epoch 11/101\n",
      "240/240 - 1s - loss: 2.4455 - accuracy: 0.1021 - val_loss: 2.4543 - val_accuracy: 0.0956 - 1s/epoch - 5ms/step\n",
      "Epoch 12/101\n",
      "240/240 - 1s - loss: 2.4420 - accuracy: 0.1021 - val_loss: 2.3935 - val_accuracy: 0.1060 - 678ms/epoch - 3ms/step\n",
      "Epoch 13/101\n",
      "240/240 - 1s - loss: 2.4395 - accuracy: 0.1041 - val_loss: 2.3696 - val_accuracy: 0.0914 - 664ms/epoch - 3ms/step\n",
      "Epoch 14/101\n",
      "240/240 - 1s - loss: 2.4389 - accuracy: 0.1006 - val_loss: 2.4082 - val_accuracy: 0.0998 - 656ms/epoch - 3ms/step\n",
      "Epoch 15/101\n",
      "240/240 - 1s - loss: 2.4413 - accuracy: 0.1034 - val_loss: 2.3870 - val_accuracy: 0.1035 - 659ms/epoch - 3ms/step\n",
      "Epoch 16/101\n",
      "240/240 - 1s - loss: 2.4421 - accuracy: 0.0984 - val_loss: 2.5584 - val_accuracy: 0.0956 - 666ms/epoch - 3ms/step\n",
      "Epoch 17/101\n",
      "240/240 - 1s - loss: 2.4416 - accuracy: 0.0996 - val_loss: 2.5991 - val_accuracy: 0.1035 - 681ms/epoch - 3ms/step\n",
      "Epoch 18/101\n",
      "240/240 - 1s - loss: 2.4423 - accuracy: 0.1014 - val_loss: 2.4010 - val_accuracy: 0.1035 - 710ms/epoch - 3ms/step\n",
      "Epoch 19/101\n",
      "240/240 - 1s - loss: 2.4421 - accuracy: 0.1006 - val_loss: 2.4494 - val_accuracy: 0.0995 - 841ms/epoch - 4ms/step\n",
      "Epoch 20/101\n",
      "240/240 - 1s - loss: 2.4406 - accuracy: 0.1028 - val_loss: 2.3837 - val_accuracy: 0.0956 - 744ms/epoch - 3ms/step\n",
      "Epoch 21/101\n",
      "240/240 - 1s - loss: 2.4430 - accuracy: 0.1018 - val_loss: 2.4393 - val_accuracy: 0.0989 - 1s/epoch - 4ms/step\n",
      "Epoch 22/101\n",
      "240/240 - 1s - loss: 2.4413 - accuracy: 0.1013 - val_loss: 2.4090 - val_accuracy: 0.0975 - 713ms/epoch - 3ms/step\n",
      "Epoch 23/101\n",
      "240/240 - 1s - loss: 2.4403 - accuracy: 0.1004 - val_loss: 2.4897 - val_accuracy: 0.0995 - 679ms/epoch - 3ms/step\n",
      "Epoch 24/101\n",
      "240/240 - 1s - loss: 2.4447 - accuracy: 0.1005 - val_loss: 2.5054 - val_accuracy: 0.1060 - 662ms/epoch - 3ms/step\n",
      "Epoch 25/101\n",
      "240/240 - 1s - loss: 2.4464 - accuracy: 0.1006 - val_loss: 2.4398 - val_accuracy: 0.0914 - 664ms/epoch - 3ms/step\n",
      "Epoch 26/101\n",
      "240/240 - 1s - loss: 2.4401 - accuracy: 0.1020 - val_loss: 2.3502 - val_accuracy: 0.0975 - 668ms/epoch - 3ms/step\n",
      "Epoch 27/101\n",
      "240/240 - 1s - loss: 2.4406 - accuracy: 0.1006 - val_loss: 2.5338 - val_accuracy: 0.0956 - 662ms/epoch - 3ms/step\n",
      "Epoch 28/101\n",
      "240/240 - 1s - loss: 2.4391 - accuracy: 0.1032 - val_loss: 2.4737 - val_accuracy: 0.0989 - 688ms/epoch - 3ms/step\n",
      "Epoch 29/101\n",
      "240/240 - 1s - loss: 2.4436 - accuracy: 0.1014 - val_loss: 2.4502 - val_accuracy: 0.1035 - 736ms/epoch - 3ms/step\n",
      "Epoch 30/101\n",
      "240/240 - 1s - loss: 2.4411 - accuracy: 0.0994 - val_loss: 2.4575 - val_accuracy: 0.0975 - 721ms/epoch - 3ms/step\n",
      "Epoch 31/101\n",
      "240/240 - 1s - loss: 2.4443 - accuracy: 0.0983 - val_loss: 2.4279 - val_accuracy: 0.1060 - 1s/epoch - 5ms/step\n",
      "Epoch 32/101\n",
      "240/240 - 1s - loss: 2.4453 - accuracy: 0.1001 - val_loss: 2.4188 - val_accuracy: 0.1081 - 722ms/epoch - 3ms/step\n",
      "Epoch 33/101\n",
      "240/240 - 1s - loss: 2.4400 - accuracy: 0.1008 - val_loss: 2.4499 - val_accuracy: 0.1081 - 692ms/epoch - 3ms/step\n",
      "Epoch 34/101\n",
      "240/240 - 1s - loss: 2.4438 - accuracy: 0.1017 - val_loss: 2.3917 - val_accuracy: 0.1035 - 694ms/epoch - 3ms/step\n",
      "Epoch 35/101\n",
      "240/240 - 1s - loss: 2.4431 - accuracy: 0.1016 - val_loss: 2.4741 - val_accuracy: 0.0956 - 719ms/epoch - 3ms/step\n",
      "Epoch 36/101\n",
      "240/240 - 1s - loss: 2.4368 - accuracy: 0.1014 - val_loss: 2.5072 - val_accuracy: 0.0914 - 689ms/epoch - 3ms/step\n",
      "Epoch 37/101\n",
      "240/240 - 1s - loss: 2.4406 - accuracy: 0.1013 - val_loss: 2.4101 - val_accuracy: 0.0998 - 802ms/epoch - 3ms/step\n",
      "Epoch 38/101\n",
      "240/240 - 1s - loss: 2.4393 - accuracy: 0.1024 - val_loss: 2.4597 - val_accuracy: 0.1081 - 704ms/epoch - 3ms/step\n",
      "Epoch 39/101\n",
      "240/240 - 1s - loss: 2.4419 - accuracy: 0.1000 - val_loss: 2.3929 - val_accuracy: 0.0997 - 680ms/epoch - 3ms/step\n",
      "Epoch 40/101\n",
      "240/240 - 1s - loss: 2.4394 - accuracy: 0.1019 - val_loss: 2.5148 - val_accuracy: 0.0975 - 693ms/epoch - 3ms/step\n",
      "Epoch 41/101\n",
      "240/240 - 1s - loss: 2.4407 - accuracy: 0.1017 - val_loss: 2.4185 - val_accuracy: 0.0995 - 1s/epoch - 5ms/step\n",
      "Epoch 42/101\n",
      "240/240 - 1s - loss: 2.4473 - accuracy: 0.0999 - val_loss: 2.4926 - val_accuracy: 0.0975 - 698ms/epoch - 3ms/step\n",
      "Epoch 43/101\n",
      "240/240 - 1s - loss: 2.4402 - accuracy: 0.1018 - val_loss: 2.4125 - val_accuracy: 0.0998 - 708ms/epoch - 3ms/step\n",
      "Epoch 44/101\n",
      "240/240 - 1s - loss: 2.4424 - accuracy: 0.1009 - val_loss: 2.4880 - val_accuracy: 0.0956 - 710ms/epoch - 3ms/step\n",
      "Epoch 45/101\n",
      "240/240 - 1s - loss: 2.4418 - accuracy: 0.1011 - val_loss: 2.4476 - val_accuracy: 0.1060 - 755ms/epoch - 3ms/step\n",
      "Epoch 46/101\n",
      "240/240 - 1s - loss: 2.4423 - accuracy: 0.1000 - val_loss: 2.4845 - val_accuracy: 0.0956 - 734ms/epoch - 3ms/step\n",
      "Epoch 47/101\n",
      "240/240 - 1s - loss: 2.4451 - accuracy: 0.0987 - val_loss: 2.4846 - val_accuracy: 0.0989 - 686ms/epoch - 3ms/step\n",
      "Epoch 48/101\n",
      "240/240 - 1s - loss: 2.4413 - accuracy: 0.1020 - val_loss: 2.3641 - val_accuracy: 0.1081 - 735ms/epoch - 3ms/step\n",
      "Epoch 49/101\n",
      "240/240 - 1s - loss: 2.4435 - accuracy: 0.1030 - val_loss: 2.3855 - val_accuracy: 0.0998 - 687ms/epoch - 3ms/step\n",
      "Epoch 50/101\n",
      "240/240 - 1s - loss: 2.4424 - accuracy: 0.0995 - val_loss: 2.3972 - val_accuracy: 0.1035 - 713ms/epoch - 3ms/step\n",
      "Epoch 51/101\n",
      "240/240 - 1s - loss: 2.4438 - accuracy: 0.1015 - val_loss: 2.3798 - val_accuracy: 0.1060 - 1s/epoch - 5ms/step\n",
      "Epoch 52/101\n",
      "240/240 - 1s - loss: 2.4451 - accuracy: 0.0993 - val_loss: 2.4493 - val_accuracy: 0.0956 - 691ms/epoch - 3ms/step\n",
      "Epoch 53/101\n",
      "240/240 - 1s - loss: 2.4439 - accuracy: 0.1014 - val_loss: 2.3996 - val_accuracy: 0.1081 - 683ms/epoch - 3ms/step\n",
      "Epoch 54/101\n",
      "240/240 - 1s - loss: 2.4403 - accuracy: 0.1030 - val_loss: 2.4655 - val_accuracy: 0.1081 - 668ms/epoch - 3ms/step\n",
      "Epoch 55/101\n",
      "240/240 - 1s - loss: 2.4393 - accuracy: 0.1027 - val_loss: 2.4654 - val_accuracy: 0.1081 - 667ms/epoch - 3ms/step\n",
      "Epoch 56/101\n",
      "240/240 - 1s - loss: 2.4445 - accuracy: 0.0994 - val_loss: 2.4055 - val_accuracy: 0.0997 - 699ms/epoch - 3ms/step\n",
      "Epoch 57/101\n",
      "240/240 - 1s - loss: 2.4409 - accuracy: 0.1038 - val_loss: 2.3943 - val_accuracy: 0.0975 - 659ms/epoch - 3ms/step\n",
      "Epoch 58/101\n",
      "240/240 - 1s - loss: 2.4400 - accuracy: 0.1021 - val_loss: 2.4187 - val_accuracy: 0.0989 - 677ms/epoch - 3ms/step\n",
      "Epoch 59/101\n",
      "240/240 - 1s - loss: 2.4439 - accuracy: 0.0991 - val_loss: 2.5079 - val_accuracy: 0.0989 - 667ms/epoch - 3ms/step\n",
      "Epoch 60/101\n",
      "240/240 - 1s - loss: 2.4472 - accuracy: 0.1008 - val_loss: 2.6032 - val_accuracy: 0.0997 - 693ms/epoch - 3ms/step\n",
      "Epoch 61/101\n",
      "240/240 - 1s - loss: 2.4454 - accuracy: 0.0997 - val_loss: 2.3791 - val_accuracy: 0.1081 - 1s/epoch - 5ms/step\n",
      "Epoch 62/101\n",
      "240/240 - 1s - loss: 2.4402 - accuracy: 0.1041 - val_loss: 2.5139 - val_accuracy: 0.1035 - 751ms/epoch - 3ms/step\n",
      "Epoch 63/101\n",
      "240/240 - 1s - loss: 2.4417 - accuracy: 0.1010 - val_loss: 2.5820 - val_accuracy: 0.1081 - 695ms/epoch - 3ms/step\n",
      "Epoch 64/101\n",
      "240/240 - 1s - loss: 2.4405 - accuracy: 0.0995 - val_loss: 2.3792 - val_accuracy: 0.1060 - 743ms/epoch - 3ms/step\n",
      "Epoch 65/101\n",
      "240/240 - 1s - loss: 2.4431 - accuracy: 0.1036 - val_loss: 2.4683 - val_accuracy: 0.0975 - 748ms/epoch - 3ms/step\n",
      "Epoch 66/101\n",
      "240/240 - 1s - loss: 2.4428 - accuracy: 0.1021 - val_loss: 2.4905 - val_accuracy: 0.0975 - 735ms/epoch - 3ms/step\n",
      "Epoch 67/101\n",
      "240/240 - 1s - loss: 2.4428 - accuracy: 0.1004 - val_loss: 2.4826 - val_accuracy: 0.0989 - 738ms/epoch - 3ms/step\n",
      "Epoch 68/101\n",
      "240/240 - 1s - loss: 2.4378 - accuracy: 0.1015 - val_loss: 2.4661 - val_accuracy: 0.1081 - 693ms/epoch - 3ms/step\n",
      "Epoch 69/101\n",
      "240/240 - 1s - loss: 2.4419 - accuracy: 0.1009 - val_loss: 2.4312 - val_accuracy: 0.0997 - 702ms/epoch - 3ms/step\n",
      "Epoch 70/101\n",
      "240/240 - 1s - loss: 2.4424 - accuracy: 0.0996 - val_loss: 2.4692 - val_accuracy: 0.1035 - 666ms/epoch - 3ms/step\n",
      "Epoch 71/101\n",
      "240/240 - 1s - loss: 2.4445 - accuracy: 0.1017 - val_loss: 2.5512 - val_accuracy: 0.1081 - 1s/epoch - 4ms/step\n",
      "Epoch 72/101\n",
      "240/240 - 1s - loss: 2.4403 - accuracy: 0.1016 - val_loss: 2.4492 - val_accuracy: 0.0989 - 663ms/epoch - 3ms/step\n",
      "Epoch 73/101\n",
      "240/240 - 1s - loss: 2.4392 - accuracy: 0.1012 - val_loss: 2.4563 - val_accuracy: 0.0914 - 775ms/epoch - 3ms/step\n",
      "Epoch 74/101\n",
      "240/240 - 1s - loss: 2.4425 - accuracy: 0.0997 - val_loss: 2.4848 - val_accuracy: 0.0995 - 799ms/epoch - 3ms/step\n",
      "Epoch 75/101\n",
      "240/240 - 1s - loss: 2.4399 - accuracy: 0.1006 - val_loss: 2.5293 - val_accuracy: 0.0914 - 771ms/epoch - 3ms/step\n",
      "Epoch 76/101\n",
      "240/240 - 1s - loss: 2.4426 - accuracy: 0.1016 - val_loss: 2.5237 - val_accuracy: 0.0989 - 751ms/epoch - 3ms/step\n",
      "Epoch 77/101\n",
      "240/240 - 1s - loss: 2.4395 - accuracy: 0.1026 - val_loss: 2.4095 - val_accuracy: 0.1081 - 782ms/epoch - 3ms/step\n",
      "Epoch 78/101\n",
      "240/240 - 1s - loss: 2.4402 - accuracy: 0.1040 - val_loss: 2.4729 - val_accuracy: 0.0989 - 776ms/epoch - 3ms/step\n",
      "Epoch 79/101\n",
      "240/240 - 1s - loss: 2.4394 - accuracy: 0.1009 - val_loss: 2.5844 - val_accuracy: 0.0956 - 714ms/epoch - 3ms/step\n",
      "Epoch 80/101\n",
      "240/240 - 1s - loss: 2.4471 - accuracy: 0.0976 - val_loss: 2.4866 - val_accuracy: 0.0995 - 726ms/epoch - 3ms/step\n",
      "Epoch 81/101\n",
      "240/240 - 1s - loss: 2.4391 - accuracy: 0.1005 - val_loss: 2.4393 - val_accuracy: 0.1035 - 1s/epoch - 5ms/step\n",
      "Epoch 82/101\n",
      "240/240 - 1s - loss: 2.4456 - accuracy: 0.0984 - val_loss: 2.4416 - val_accuracy: 0.1035 - 732ms/epoch - 3ms/step\n",
      "Epoch 83/101\n",
      "240/240 - 1s - loss: 2.4387 - accuracy: 0.1012 - val_loss: 2.4308 - val_accuracy: 0.1060 - 728ms/epoch - 3ms/step\n",
      "Epoch 84/101\n",
      "240/240 - 1s - loss: 2.4409 - accuracy: 0.1024 - val_loss: 2.4436 - val_accuracy: 0.0997 - 707ms/epoch - 3ms/step\n",
      "Epoch 85/101\n",
      "240/240 - 1s - loss: 2.4461 - accuracy: 0.1022 - val_loss: 2.4237 - val_accuracy: 0.0995 - 728ms/epoch - 3ms/step\n",
      "Epoch 86/101\n",
      "240/240 - 1s - loss: 2.4361 - accuracy: 0.1043 - val_loss: 2.4980 - val_accuracy: 0.0975 - 826ms/epoch - 3ms/step\n",
      "Epoch 87/101\n",
      "240/240 - 1s - loss: 2.4435 - accuracy: 0.1005 - val_loss: 2.4311 - val_accuracy: 0.0956 - 756ms/epoch - 3ms/step\n",
      "Epoch 88/101\n",
      "240/240 - 1s - loss: 2.4404 - accuracy: 0.1032 - val_loss: 2.4550 - val_accuracy: 0.0914 - 729ms/epoch - 3ms/step\n",
      "Epoch 89/101\n",
      "240/240 - 1s - loss: 2.4438 - accuracy: 0.1006 - val_loss: 2.4740 - val_accuracy: 0.0995 - 714ms/epoch - 3ms/step\n",
      "Epoch 90/101\n",
      "240/240 - 1s - loss: 2.4409 - accuracy: 0.1033 - val_loss: 2.5692 - val_accuracy: 0.1035 - 735ms/epoch - 3ms/step\n",
      "Epoch 91/101\n",
      "240/240 - 1s - loss: 2.4464 - accuracy: 0.1004 - val_loss: 2.3946 - val_accuracy: 0.0975 - 1s/epoch - 5ms/step\n",
      "Epoch 92/101\n",
      "240/240 - 1s - loss: 2.4387 - accuracy: 0.1011 - val_loss: 2.5463 - val_accuracy: 0.0956 - 870ms/epoch - 4ms/step\n",
      "Epoch 93/101\n",
      "240/240 - 1s - loss: 2.4435 - accuracy: 0.0999 - val_loss: 2.3646 - val_accuracy: 0.0975 - 767ms/epoch - 3ms/step\n",
      "Epoch 94/101\n",
      "240/240 - 1s - loss: 2.4388 - accuracy: 0.1018 - val_loss: 2.4384 - val_accuracy: 0.1060 - 734ms/epoch - 3ms/step\n",
      "Epoch 95/101\n",
      "240/240 - 1s - loss: 2.4424 - accuracy: 0.0999 - val_loss: 2.4192 - val_accuracy: 0.1081 - 776ms/epoch - 3ms/step\n",
      "Epoch 96/101\n",
      "240/240 - 1s - loss: 2.4401 - accuracy: 0.1018 - val_loss: 2.3858 - val_accuracy: 0.0975 - 757ms/epoch - 3ms/step\n",
      "Epoch 97/101\n",
      "240/240 - 1s - loss: 2.4396 - accuracy: 0.1017 - val_loss: 2.5199 - val_accuracy: 0.0998 - 829ms/epoch - 3ms/step\n",
      "Epoch 98/101\n",
      "240/240 - 1s - loss: 2.4417 - accuracy: 0.1017 - val_loss: 2.4414 - val_accuracy: 0.0975 - 812ms/epoch - 3ms/step\n",
      "Epoch 99/101\n",
      "240/240 - 1s - loss: 2.4442 - accuracy: 0.0988 - val_loss: 2.3888 - val_accuracy: 0.0997 - 839ms/epoch - 3ms/step\n",
      "Epoch 100/101\n",
      "240/240 - 1s - loss: 2.4453 - accuracy: 0.0999 - val_loss: 2.4418 - val_accuracy: 0.0998 - 799ms/epoch - 3ms/step\n",
      "Epoch 101/101\n",
      "240/240 - 1s - loss: 2.4431 - accuracy: 0.1003 - val_loss: 2.4201 - val_accuracy: 0.1035 - 1s/epoch - 5ms/step\n",
      "1875/1875 - 2s - loss: 2.4130 - accuracy: 0.1022 - 2s/epoch - 1ms/step\n",
      "313/313 - 0s - loss: 2.4101 - accuracy: 0.1010 - 312ms/epoch - 997us/step\n"
     ]
    }
   ],
   "source": [
    "callback1 = GradientCallback()\n",
    "callback2 = tf.keras.callbacks.TensorBoard(\n",
    "  log_dir=log_dir,\n",
    "  histogram_freq=10\n",
    ")\n",
    "\n",
    "ret = model.fit(x_train,y_train,epochs=101,batch_size=200,\n",
    "                validation_split=0.2,verbose=2,callbacks=[callback1,callback2])\n",
    "train_loss,train_acc = model.evaluate(x_train,y_train,verbose=2)\n",
    "test_loss,test_acc = model.evaluate(x_test,y_test,verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_8 (Flatten)         (None, 3072)              0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 100)               307300    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 318,410\n",
      "Trainable params: 318,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_8\" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(400, 28, 28)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21064\\1925912333.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m ret = model.fit(x_train,y_train,epochs=201,batch_size=400,\n\u001b[0m\u001b[0;32m     22\u001b[0m                 validation_data = (x_test,y_test),verbose=0)\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_8\" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(400, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# 드롭아웃 비율 0.2로 학습시키기\n",
    "act = tf.keras.layers.LeakyReLU(alpha=0.3)\n",
    "init = 'he_uniform'\n",
    "n = 100\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(32,32,3)))\n",
    "model.add(tf.keras.layers.Dense(units=n,activation=act,kernel_initializer=init))\n",
    "model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units=n,activation=act,kernel_initializer=init))\n",
    "model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(units=10,activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "ret = model.fit(x_train,y_train,epochs=201,batch_size=400,\n",
    "                validation_data = (x_test,y_test),verbose=0)\n",
    "\n",
    "train_loss, train_acc = model.evaluate(x_train,y_train,verbose=2)\n",
    "test_loss, test_acc = model.evaluate(x_test,y_test,verbose=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
